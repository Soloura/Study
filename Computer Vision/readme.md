# Computer Vision

## ğŸŒ³ Object Detection ğŸŒ³

### *LeNet: Gradient-Based Learning Applied to Document Recognition* | [Homepage](http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf)
LeNetì€ Yann LeCunì´ 1998ì— ì œì•ˆí•œ Convolutional Neural Network(CNN) ëª¨ë¸ì´ë‹¤. LeNetì€ ì†ê¸€ì”¨ë¡œ ëœ ìš°í¸ ë²ˆí˜¸(ìˆ«ì)ë¥¼ ì¸ì‹í•œë‹¤. ê¸°ì¡´ì˜ Fully Connected(FC)ë¥¼ ê°œì„ í•˜ê³ ì ì—°êµ¬ë˜ì—ˆë‹¤. ImageëŠ” spatial structure, informationì„ ê°–ëŠ”ë°, FC layerì— í†µê³¼ì‹œí‚¤ê¸° ìœ„í•´ flatten ì‘ì—…ì„ ê±°ì¹˜ë©´ topology dataë¥¼ ìƒê²Œ ëœë‹¤. LeNetì€ local receptive field, shared weight, sub samplingì„ ê²°í•©í•œ convoltuional layerì„ ì´ìš©í•œë‹¤. LeNet-1ë¶€í„° LeNet-5ì´ ì—°êµ¬ ë° ê°œë°œë˜ì—ˆëŠ”ë°, ì°¨ì´ëŠ” convolution kernel/filterì˜ ê°œìˆ˜ë¥¼ ëŠ˜ë¦¬ê³  ë§ˆì§€ë§‰ FC layer í¬ê¸°ë¥¼ í‚¤ì› ë‹¤. LeNet-1ì€ input-convolution-subsampling-convolution-subsampling-convolution-outputì´ë‹¤. LeNet-5ëŠ” Input-C1(Convolution)-S2(Subsampling)-C3(Convolution)-S4(Subsampling)-C5(Full connection)-F6(Full connection)-OUTPUT(Gaussian connection)ì´ë‹¤.

### *AlexNet: ImageNet Classification with Deep Convolutional Neural Networks* | [NIPS](https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf)
2012ë…„ ImageNet ILSVRCì—ì„œ 1ìœ„ë¥¼ í•˜ë©° CNNì„ ë„ë¦¬ ì•Œë¦¬ê²Œ ëœ ëª¨ë¸ë¡œ, ì£¼ë¡œ convolutional layer ë‹¤ìŒì— pooling layerê°€ ì˜¤ëŠ” êµ¬ì¡°ì™€ ë‹¬ë¦¬ convolutional layerê°€ ì˜¤ë„ë¡ êµ¬ì„±í–ˆë‹¤.

### *ZFNet: Visualizing and Understanding Convolutional Networks* | [Homepage](https://cs.nyu.edu/~fergus/papers/zeilerECCV2014.pdf)
2013ë…„ ImageNet ILSVRCì—ì„œ 1ìœ„ë¥¼ í•œ ëª¨ë¸ë¡œ, NYUì˜ Matthew Zeilerì™€ Rob Fergusì˜ ì„± ì•ê¸€ìë¥¼ í•˜ë‚˜ì”© ë”°ì„œ ì´ë¦„ì´ ë¶™ì—ˆê³ , ì¤‘ê°„ convolutional layerì˜ í¬ê¸°ë¥¼ ëŠ˜ë¦° êµ¬ì¡°ì´ë‹¤.

### *NIN: Network In Network* | [arXiv](https://arxiv.org/abs/1312.4400)
ë„¤íŠ¸ì›Œí¬ ì†ì˜ ë„¤íŠ¸ì›Œí¬ë¡œ, ê¸°ì¡´ì˜ CNNì˜ linear convolution layerì™€ ë‹¬ë¦¬ filter ëŒ€ì‹ ì— MLP(Multi-Layer Perceptron)ì„ ì‚¬ìš©í•˜ë©° non-linearí•œ ì„±ì§ˆì„ ì´ìš©í•´ì„œ feature ì¶”ì¶œì„ í•œë‹¤. MLP Convolutional layer ì—¬ëŸ¬ ê°œë¥¼ networkë¡œ ë§Œë“¤ì–´ì„œ ì‚¬ìš©í•˜ê¸° ë•Œë¬¸ì— network in networkì´ë‹¤. ë˜í•œ 1x1 convolutionì„ ì´ìš©í•˜ì—¬ feature mapì„ ì¤„ì˜€ë‹¤. 1x1 convolutionì€ neurons that fire together, wire togetherì¸ Hebbian principleì™€ ê°™ì´ ì—¬ëŸ¬ ê°œì˜ feature mapì—ì„œ ë¹„ìŠ·í•œ ì„±ì§ˆì„ ë¬¶ì„ ìˆ˜ ìˆì–´ ìˆ«ìë¥¼ ì¤„ì—¬ ì—°ì‚°ëŸ‰ì„ ì¤„ì¼ ìˆ˜ ìˆë‹¤. ê·¸ë¦¬ê³  ê¸°ì¡´ CNNì™€ ë‹¬ë¦¬ ë§ˆì§€ë§‰ layerì— fully connected layerê°€ ì•„ë‹Œ global average poolingì„ classifierë¡œ ì‚¬ìš©í•˜ì—¬ overfittingê³¼ ì—°ì‚°ì„ ì¤„ì¸ë‹¤.

### *Auxiliary Classifier: Training Deeper Convolutional Networks with Deep SuperVision* | [arXiv](https://arxiv.org/abs/1505.02496)
Auxiliary Classifier blockì„ ì´ìš©í•˜ë©´ backpropagation ë•Œ ê²°ê³¼ë¥¼ í•©ì¹˜ê¸°ì— gradientê°€ ì‘ì•„ì§€ëŠ” ë¬¸ì œë¥¼ í”¼í•  ìˆ˜ ìˆë‹¤.

### *GoogLeNet: Going deeper with convolutions* | [arXiv](https://arxiv.org/abs/1409.4842)
2014ë…„ ImageNet ILSVRCì—ì„œ 1ìœ„í•œ Googleì—ì„œ ë§Œë“  ëª¨ë¸ë¡œ, Inception moduleì˜ ê°œë…ì„ ë§Œë“¤ì—ˆìœ¼ë©°, ì´ë¥¼ í†µí•´ parameterë¥¼ AlexNet 60Mì—ì„œ GoogLeNetì„ 4Mìœ¼ë¡œ ì¤„ì˜€ë‹¤. 1x1 convolution, NIN, Inception moduleì„ ì‚¬ìš©í•˜ì—¬ ì—°ì‚°ëŸ‰ì„ ìœ ì§€í•˜ë©´ì„œ networkë¥¼ ê¹Šê³  ë„“ê²Œ ë§Œë“¤ì—ˆë‹¤. Auxiliary classifier block unitì„ í†µí•´ vanishing gradientë¥¼ í”¼í•œë‹¤. 

### *VGGNet: Very Deep Convolutional Networks for Large-Scale Image Recognition* | [arXiv](https://arxiv.org/abs/1409.1556)
2014ë…„ ImageNet ILSVRCì—ì„œ 2ìœ„í•œ Oxford Universityì—ì„œ ë§Œë“  ëª¨ë¸ë¡œ depthì— ë”°ë¥¸ ì˜í–¥ì„ ë‚˜íƒ€ëƒˆë‹¤. ì‹œì‘ë¶€í„° ëê¹Œì§€ 3x3 convolutionê³¼ 2x2 max poolingì„ ì‚¬ìš©í•˜ëŠ” homogeneous êµ¬ì¡°ì—ì„œ depthê°€ 16ì¼ ë•Œ ìµœì ì˜ ê²°ê³¼ê°€ ë‚˜ì˜¤ë©°, ë¶„ë¥˜ ì„±ëŠ¥ì€ GoogLeNetì— ë¹„í•´ ì„±ëŠ¥ì€ ë¶€ì¡±í•˜ì§€ë§Œ ë‹¤ì¤‘ ì „ë‹¬ í•™ìŠµ ê³¼ì œì—ì„œëŠ” ì„±ëŠ¥ì´ ìš°ì›”í–ˆë‹¤. ë©”ëª¨ë¦¬, parameterê°€ í¬ë‹¤ëŠ” ë‹¨ì ì´ ìˆë‹¤.

### *ResNet: Deep Residual Learning for Image Recognition* | [CVPR](https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf)
2015ë…„ ImageNet ILSVRCì—ì„œ 1ìœ„ë¥¼ í•œ Microsoftì—ì„œ ë§Œë“  ëª¨ë¸ë¡œ, layer ìˆ˜ê°€ Deep ë³´ë‹¤ ë§ì€ Deeperí•œ ë„¤íŠ¸ì›Œí¬ì— ëŒ€í•´ì„œ í•™ìŠµì„ í•˜ëŠ” residual framework/moduleì„ ì†Œê°œí–ˆë‹¤.

### *ResNeXt, Aggregated Residual Transformations for Deep Neural Networks* | [arXiv](https://arxiv.org/abs/1611.05431)
2016ë…„ UCSDì™€ Facebookì—ì„œ ì œì•ˆí•œ ResNeXtëŠ” ResNetì˜ ë³€í˜• networkì´ë‹¤. Inputì„ *group convolution*ì„ í†µí•´ ì—¬ëŸ¬ê°œë¡œ ë‚˜ëˆ„ê³  1x1 convolutionìœ¼ë¡œ inputì„ transformí•˜ê³ , concatë¥¼ í†µí•´ mergeí•œë‹¤(*Split-Transform-Merge*). ResNetì— ë¹„í•´ *parameterë¥¼ ì¤„ì—¬ ì—°ì‚°ëŸ‰ì„ ì¤„ì´ê³ * ë” ë†’ì€ ì„±ëŠ¥ì„ ë³´ì˜€ë‹¤. Networkì—ì„œ ê° convolutional layerë¥¼ ì§€ë‚  ë•Œ ë§ˆë‹¤ outputì˜ í¬ê¸°ê°€ 1/2ë¡œ ì¤„ì–´ë“ ë‹¤. ResNetì€ í•˜ë‚˜ì˜ convolutional layerì„ í†µí•´ deepí•˜ê²Œ ë§Œë“¤ì—ˆì§€ë§Œ, ResNeXtëŠ” ì¡°ê¸ˆ ë” ê¹Šì§€ë§Œ group convolutionì„ í†µí•´ ì—°ì‚°ëŸ‰ì„ ë‚®ì·„ë‹¤. ë…¼ë¬¸ì—ì„œ ë‚˜ì˜¨ CëŠ” *cardinarity*ë¡œ group convolutionì˜ ìˆ˜(the size of the set of transformation)ì´ë‹¤. ResNetì—£ëŠ” 50 ì´í•˜ depthì¼ ë•ŒëŠ” block 1ê°œ, convolutionì„ 2ê°œë§Œ ì—°ì‚°í–ˆë‹¤. í•˜ì§€ë§Œ ResNeXtì—ì„œëŠ” 2ê°œì˜ blockì€ group convolutionì˜ íš¨ê³¼ê°€ ì—†ì–´ì„œ block depthê°€ 3 ì´ìƒì¼ ë•Œë¶€í„° íš¨ê³¼ê°€ ìˆë‹¤. Cardinalityì˜ í¬ê¸°ë¥¼ í‚¤ìš¸ìˆ˜ë¡(group ìˆ˜ê°€ ë§ì•„ì§ˆìˆ˜ë¡) parameterë¥¼ ì¤„ì—¬ ì—°ì‚°ëŸ‰ì„ ì¤„ì¼ ìˆ˜ ìˆë‹¤. ì¦‰, ê°™ì€ parameterì¼ ë•Œ ë” ë§ì€ channel ì´ìš©í•´ì„œ deeper network ì„¤ê³„ê°€ ê°€ëŠ¥í•˜ë‹¤.

### *DenseNet: Densely Connected Convolutional Networks* | [IEEE](https://ieeexplore.ieee.org/document/8099726) | [arXiv](https://arxiv.org/abs/1608.06993)
Huangì´ ì œì•ˆí•œ ResNetì˜ í™•ì¥íŒìœ¼ë¡œ ResNet ë¸”ë¡ì—ì„œëŠ” í•©ì‚°ì„ í†µí•´ ì´ì „ layerì™€ í˜„ì¬ layerê°€ í•©ì³ì¡Œë‹¤. DenseNetì˜ ê²½ìš°, ì—°ê²°ì„ í†µí•´ í•©ì³ì§„ë‹¤. ëª¨ë“  layerë¥¼ ì´ì „ layerì™€ ì—°ê²°í•˜ê³  í˜„ì¬ layerë¥¼ ë‹¤ìŒ layerì— ì—°ê²°í•œë‹¤. ì´ë¥¼ í†µí•´ ë” ë§¤ë„ëŸ¬ìš´ ê¸°ìš¸ê¸°, íŠ¹ì§• ë³€í™˜ ë“±ê³¼ ê°™ì€ ì—¬ëŸ¬ ê°€ì§€ ì´ì ì„ ì œê³µí•œë‹¤. ë˜í•œ parameterì˜ ê°œìˆ˜ê°€ ì¤„ì–´ë“ ë‹¤.

### *MobileNet: Efficient Convolutional Neural Networks for Mobile Vision Application* | [arXiv](https://arxiv.org/abs/1704.04861)
MobileNetì€ Googleì—ì„œ ì—°êµ¬í•œ Networkë¡œ version 1, 2, 3ì€ ê° 2017, 2018, 2019ì— ë°œí‘œë˜ì—ˆë‹¤. ê³ ì„±ëŠ¥ì˜ deviceê°€ ì•„ë‹Œ vehicle, drone smart phoneê³¼ ê°™ì€ í™˜ê²½ì—ì„œëŠ” computing power, memoryê°€ ë¶€ì¡±í•˜ë‹¤. ë”°ë¼ì„œ battery performanceê°€ ì¤‘ìš”í•œ ê³³ì„ ëª©ì ìœ¼ë¡œ ì„¤ê³„ëœ CNNì´ë‹¤. ì‘ì€ neural networkë¥¼ ë§Œë“œëŠ” ë°©ë²•ì—ëŠ” 1. remove fully-connected layersìœ¼ë¡œ CNN parameters 90%ë¥¼ FC layersê°€ ì°¨ì§€í•œë‹¤. 2. kernel reductionìœ¼ë¡œ 3x3ì„ 1x1ìœ¼ë¡œ ë³€ê²½í•´ì„œ ì—°ì‚°ëŸ‰ì„ ì¤„ì¸ë‹¤. 3. channel reduction. 4. evenly spaced downsamplingìœ¼ë¡œ ì´ˆë°˜ì— downsamplingì„ ë§ì´ í•˜ë©´ accuracyê°€ ë–¨ì–´ì§€ì§€ë§Œ parameterê°€ ì ì–´ì§€ê³  í›„ë°˜ì— downsamplingì„ ë§ì´ í•˜ë©´ accuracyê°€ ì¢‹ì•„ì§€ì§€ë§Œ parameterê°€ ë§ì•„ì§€ê¸° ë•Œë¬¸ì— ì ë‹¹íˆ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ì¢‹ë‹¤. 5. depthwise seperable convolutions. 6. shuffle operations. 7. distillation & compression.

ê¸°ì¡´ì˜ CNNì€ HxW í¬ê¸°ì˜ Cê°œì˜ ì±„ë„ imageì— KxKxC í¬ê¸°ì˜ Mê°œ filterë¥¼ ê³±í•˜ì—¬ H'xW' í¬ê¸°ì˜ M ì±„ë„ì˜ imageë¥¼ ìƒì„±í•œë‹¤. Depthwise & Pointwise convolutionì€ ì´ì™€ ë‹¬ë¦¬ í•œ ë°©í–¥ìœ¼ë¡œë§Œ í¬ê¸°ë¥¼ ì¤„ì´ëŠ” ì „ëµì´ë‹¤. Depthwise convolutionì€ channel ê°œìˆ˜ëŠ” ì¤„ì–´ë“¤ì§€ ì•Šê³  1ê°œì˜ channelì—ì„œì˜ í¬ê¸°ë§Œ ì¤„ì–´ë“ ë‹¤. Pointwise convolutionì€ channel ìˆ«ìê°€ 1ê°œë¡œ ì¤„ì–´ë“ ë‹¤. ê¸°ì¡´ CNNì˜ parameter ìˆ˜ëŠ” K^2CM, ê³„ì‚°ëŸ‰ì€ K^2CMH'W'ì´ë‹¤. Depthwise convoltuionê³¼ Pointwise convolutionì„ í•©í•œ parameterëŠ” K^2C+CM, ê³„ì‚°ëŸ‰ì€ K^2CW'H' + CMW"H"ì´ë‹¤. ë§Œì•½ W'=W", H'=H"ì´ë©´ W'H'C(K^2+M)ì´ë‹¤. ì¦‰, Depthwise convolutionê³¼ pointwise convolutionì„ í•©ì¹œ Separable convolutionsì˜ ê³„ì‚°ëŸ‰ì€ ê¸°ì¡´ CNNì— ë¹„í•´ì„œ (1/M + 1/K^2)ìœ¼ë¡œ K=3ì¼ ê²½ìš° ì•½ 8~9ë°°ì˜ íš¨ìœ¨ì„ ë³´ì¸ë‹¤. (H=H'=H", W=W'=W"d ì¼ ë•Œ)

## ğŸŒ³ Semantic Segmentation ğŸŒ³

### *R-CNN: Rich feature hierarchies for accurate object detection and semantic segmentation* | [arXiv](https://arxiv.org/abs/1311.2524)
2013ë…„ UC Berkeleyì—ì„œ ë°œí‘œí•œ object detection, semantic segmentation modelì´ë‹¤. ì´ë¯¸ì§€ë¥¼ ë¶„ë¥˜í•˜ëŠ” ê²ƒë³´ë‹¤ ì´ë¯¸ì§€ ì•ˆì— objectì¸ì§€ êµ¬ë¶„í•˜ëŠ” ê²ƒì´ ì–´ë ¤ìš´ ì‘ì—…ì´ë‹¤. R-CNNì€ ì´ë¥¼ ìœ„í•´ ëª‡ ë‹¨ê³„ë¥¼ ê±°ì¹œë‹¤. ë¨¼ì € í›„ë³´ ì´ë¯¸ì§€ ì˜ì—­ì„ ì°¾ì•„ë‚´ëŠ” region proposal/bounding boxë¥¼ ì°¾ëŠ” ë‹¨ê³„ê°€ ìˆë‹¤. Bounding boxë¥¼ ì°¾ê¸° ìœ„í•´ ìƒ‰ìƒì´ë‚˜ íŒ¨í„´ ë“±ì´ ë¹„ìŠ·í•œ ì¸ì ‘í•œ í”½ì…€ì„ í•©ì¹˜ëŠ” selective search ê³¼ì •ì„ ê±°ì¹œë‹¤. ë‹¤ìŒ ì¶”ì¶œí•œ bounding boxë¥¼ CNNì˜ ì…ë ¥ìœ¼ë¡œ ë„£ê¸° ìœ„í•´ ê°•ì œë¡œ ì‚¬ì´ì¦ˆë¥¼ í†µì¼ ì‹œí‚¨ë‹¤. ì´ ë•Œ CNNì€ í›ˆë ¨ëœ AlexNetì˜ ë³€í˜•ëœ ë²„ì „ì´ë‹¤. CNNì˜ ë§ˆì§€ë§‰ ë‹¨ê³„ì—ì„œ Support Vector Machine(SVM)ì„ ì‚¬ìš©í•˜ì—¬ ì´ë¯¸ì§€ë¥¼ ë¶„ë¥˜í•œë‹¤. ê·¸ë¦¬ê³  ìµœì¢…ì ìœ¼ë¡œ ë¶„ë¥˜ëœ objectì˜ bounding box ì¢Œí‘œë¥¼ ë” ì •í™•íˆ ë§ì¶”ê¸° ìœ„í•´ linear regression modelì„ ì‚¬ìš©í•œë‹¤.

### *Fast R-CNN* | [arXiv](https://arxiv.org/abs/1504.08083)
R-CNNì˜ ë¬¸ì œì ì€ ëª¨ë“  bounding boxì— ëŒ€í•´ CNN, SVM, linear regression 3ê°€ì§€ ëª¨ë¸ì„ í›ˆë ¨ì‹œì¼œì•¼í•˜ê¸° ë–„ë¬¸ì— ì–´ë µë‹¤. ë•Œë¬¸ì— Fast R-CNNì€ bounding box ì‚¬ì´ì— ê²¹ì¹˜ëŠ” ì˜ì—­ì´ CNNì„ í†µê³¼ì‹œí‚¤ëŠ” ê²ƒì€ ë‚­ë¹„ë¼ ìƒê°í–ˆë‹¤. Region of Interset Pooling(RolPool)ì˜ ê°œë…ì„ ë„ì…í•˜ì—¬ selective searchì—ì„œ ì°¾ì€ bounding box ì •ë³´ë¥¼ CNNì„ í†µê³¼ì‹œí‚¤ë©´ì„œ ìœ ì§€ì‹œí‚¤ê³  ìµœì¢… CNN feature mapìœ¼ë¡œë¶€í„° í•´ë‹¹ ì˜ì—­ì„ ì¶”ì¶œí•˜ì—¬ poolingí•œë‹¤. ì´ë¥¼ í†µí•´ bounding boxë§ˆë‹¤ CNNì„ ê±°ì¹˜ëŠ” ì‹œê°„ì„ ë‹¨ì¶•ì‹œí‚¨ë‹¤. ë˜í•œ SVM ëŒ€ì‹  CNN ë’¤ì— softmaxë¥¼ ë†“ê³  linear regression ëŒ€ì‹  softmax layerì™€ ë™ì¼í•˜ê²Œ ë’¤ì— ì¶”ê°€í–ˆë‹¤. Joint the feature extractor, classifier, regressor together in a unified framework.

### *Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks* | [arXiv](https://arxiv.org/abs/1506.01497) | [PyCaffe](https://github.com/rbgirshick/py-faster-rcnn) | [PyTorch](https://github.com/longcw/faster_rcnn_pytorch) | [MatLab](https://github.com/ShaoqingRen/faster_rcnn)
Fast R-CNNì—ì„œ ë‚¨ì€ bottleneckì€ bounding boxë¥¼ ë§Œë“œëŠ” region proposal ë‹¨ê³„ì´ë‹¤. Faster R-CNNì€ region proposal ë‹¨ê³„ë¥¼ CNN ì•ˆì— ë„£ì–´ì„œ ë¬¸ì œë¥¼ í•´ê²°í–ˆë‹¤. CNNì„ í†µê³¼í•œ feature mapì—ì„œ sliding windowë¥¼ ì´ìš©í•´ ê° anchorë§ˆë‹¤ ê°€ëŠ¥í•œ bounding boxì˜ ì¢Œí‘œì™€ ì´ bounding boxì˜ ì ìˆ˜ë¥¼ ê³„ì‚°í•œë‹¤. ëŒ€ë¶€ë¶„ ë„ˆë¬´ í™€ì­‰í•˜ê±°ë‚˜ ë„“ì€ ë¬¼ì²´ëŠ” ë§ì§€ ì•Šìœ¼ë¯€ë¡œ 2:1, 1:1, 1:2 ë“±ì˜ ëª‡ê°€ì§€ íƒ€ì…ìœ¼ë¡œë„ ì¢‹ë‹¤. Faster R-CNNì€ Microsoftì—ì„œ ì—°êµ¬í•œ ë‚´ìš©ì´ë‹¤.

### *Mask R-CNN* | [arXiv](https://arxiv.org/abs/1703.06870) | [PyTorch](https://github.com/felixgwu/mask_rcnn_pytorch) | [TesforFlow](https://github.com/CharlesShang/FastMaskRCNN)
ë¶„í• ëœ imageë¥¼ maskingí•˜ëŠ” Mask R-CNNì€ Facebookì—ì„œ ì—°êµ¬í•œ ë‚´ìš©ìœ¼ë¡œ, ê° í”½ì…€ì´ objectì— í•´ë‹¹í•˜ëŠ” ê²ƒì¸ì§€ ì•„ë‹Œì§€ë¥¼ maskingí•˜ëŠ” networkë¥¼ ì¶”ê°€í–ˆë‹¤. ì´ëŠ” binary maskë¼ í•œë‹¤. ì •í™•í•œ í”½ì…€ ìœ„ì¹˜ë¥¼ ì¶”ì¶œí•˜ê¸° ìœ„í•´ CNNì„ í†µê³¼í•˜ë©´ì„œ RolPool ì˜ì—­ì— ìœ„ì¹˜ì— ìƒê¸°ëŠ” ì†Œìˆ«ì  ì˜¤ì°¨ë¥¼ 2D bilinear interpolationì„ í†µí•´ ê°ì†Œì‹œì¼°ë‹¤. ì´ëŠ” RolAlignì´ë‹¤. 

### *YOLO: You Only Look Once, Real-Time Object Detection* | [Homepage](https://pjreddie.com/darknet/yolo/)
YOLOëŠ” ì´ë¯¸ì§€ ë‚´ì˜ bounding boxì™€ class probailityë¥¼ single regression problemìœ¼ë¡œ unifiedí•˜ì—¬ ì´ë¯¸ì§€ë¥¼ í•œë²ˆ ë³´ëŠ” ê²ƒìœ¼ë¡œ objectì˜ ì¢…ë¥˜ì™€ ìœ„ì¹˜ë¥¼ ì¶”ì¸¡í•œë‹¤. Single convolutional networkë¥¼ í†µí•´ multiple bounding boxì— ëŒ€í•œ class probabilityë¥¼ ê³„ì‚°í•˜ëŠ” ë°©ì‹ì´ë‹¤. ê¸°ì¡´ì˜ object detection methodì™€ ë¹„êµí–ˆì„ ë•Œ, YOLOì˜ ìƒëŒ€ì ì¸ ì¥ì ê³¼ ë‹¨ì ì€ ë‹¤ìŒê³¼ ê°™ë‹¤. ì¥ì ìœ¼ë¡œëŠ” 1. ê°„ë‹¨í•œ ì²˜ë¦¬ê³¼ì •ìœ¼ë¡œ ì†ë„ê°€ ë§¤ìš° ë¹ ë¥´ê³  ê¸°ì¡´ì˜ ë‹¤ë¥¸ real-time detection systemë“¤ê³¼ ë¹„êµí•  ë•Œ 2ë°° ì •ë„ ë†’ì€ mAPë¥¼ ë³´ì¸ë‹¤. 2. Image ì „ì²´ë¥¼ 1ë²ˆì— ë°”ë¼ë³´ëŠ” ë°©ì‹ìœ¼ë¡œ classì— ëŒ€í•œ ë§¥ë½ì  ì´í•´ë„ê°€ ë†’ì•„ ë‚®ì€ background error(false-negative)ë¥¼ ë³´ì¸ë‹¤. 3. Objectì— ëŒ€í•œ ì¢€ ë” ì¼ë°˜í™”ëœ íŠ¹ì§•ì„ í•™ìŠµí•˜ê¸° ë•Œë¬¸ì— natural imageë¡œ í•™ìŠµí•˜ê³  artworkì— í…ŒìŠ¤íŠ¸í•´ë„ ë‹¤ë¥¸ detection systemì— ë¹„í•´ í›¨ì”¬ ë†’ì€ ì„±ëŠ¥ì„ ë³´ì¸ë‹¤. ë‹¨ì ìœ¼ë¡œëŠ” ìƒëŒ€ì ìœ¼ë¡œ ì •í™•ë„ê°€ ë‚®ì€ë°, íŠ¹íˆ ì‘ì€ objectì¼ ìˆ˜ë¡ì´ë‹¤.

Unified Detectionì€ input imageë¥¼ S by S gridë¡œ ë‚˜ëˆˆë‹¤. ê°ê°ì˜ grid cellì€ Bê°œì˜ bounding boxì™€ ê° bounding boxì— ëŒ€í•œ confidence scoreë¥¼ ê°–ëŠ”ë‹¤. ë§Œì•½ cellì— objectê°€ ì—†ë‹¤ë©´ confidence scoreëŠ” 0ì´ ëœë‹¤. ê°ê°ì˜ grid cellì€ Cê°œì˜ conditional class probabilityë¥¼ ê°–ëŠ”ë‹¤. ê°ê°ì˜ bounding boxëŠ” x, y, w, h, confidenceë¡œ êµ¬ì„±ëœë‹¤. (x, y)ëŠ” bounding boxì˜ ì¤‘ì‹¬ì ì„ ì˜ë¯¸í•˜ë©° grid cellì˜ ë²”ìœ„ì— ëŒ€í•œ ìƒëŒ€ê°’ì´ ì…ë ¥ëœë‹¤. (w, h) ì „ì²´ imageì˜ width, heightì— ëŒ€í•œ ìƒëŒ€ê°’ì´ ì…ë ¥ëœë‹¤. Test timeì—ëŠ” conditional class probabilityì™€ bounding boxì˜ confidence scoreë¥¼ ê³±í•˜ì—¬ class-specific confidence scoreë¥¼ ì–»ëŠ”ë‹¤. ë…¼ë¬¸ì—ì„œëŠ” YOLOì˜ ì„±ëŠ¥í‰ê°€ë¥¼ ìœ„í•´ PASCAL VOCë¥¼ ì‚¬ìš©í–ˆìœ¼ë©° S, B, Cì—ëŠ” ê°ê° 7, 2, 20ì„ ì‚¬ìš©í–ˆë‹¤.

Network Design/Architectureì€ GoogLeNet ëª¨ë¸ì˜ 24 convolutional layers and 2 fully connected layersì„ ê¸°ë°˜ìœ¼ë¡œ 24 convolutional layersë¥¼ 9ê°œë¡œ ëŒ€ì²´í–ˆë‹¤. ê³„ì‚°ì„ ë§ˆì¹˜ë©´ ì´ 98ê°œì˜ class specific confidence scoreë¥¼ ì–»ê²Œ ë˜ê³ , ì´ì— ëŒ€í•´ ê° 20ê°œì˜ classë¥¼ ê¸°ì¤€ìœ¼ë¡œ non-maximum suppressionì„ í•˜ì—¬ objectì— ëŒ€í•œ class ë° bounding box locationì„ ê²°ì •í•œë‹¤. 

Loss functionì€ gird cellì˜ ì—¬ëŸ¬ bounding box ì¤‘ ground truth boxì™€ì˜ IOUê°€ ê°€ì¥ ë†’ì€ bounding boxë¥¼ predictorë¡œ ì„¤ì •í•œë‹¤. objectê°€ ì¡´ì¬í•˜ëŠ” grid cell iì˜ predictor bounding box j, objectê°€ ì¡´ì¬í•˜ì§€ ì•ŠëŠ” grid cell iì˜ bounding box j, objectê°€ ì¡´ì¬í•˜ëŠ” grid cell iì„ ê¸°í˜¸ë¡œ ì‚¬ìš©í•˜ê³ , ground truth boxì˜ ì¤‘ì‹¬ì ì´ ì–´ë–¤ grid cell ë‚´ë¶€ì— ìœ„ì¹˜í•˜ë©´, ê·¸ grid cellì—ëŠ” objectê°€ ì¡´ì¬í•œë‹¤ê³  ì—¬ê¸´ë‹¤. [ì°¸ê³ ](https://curt-park.github.io/2017-03-26/yolo/)

YOLOì˜ í•œê³„ëŠ” 1. ê° grid cellì´ í•˜ë‚˜ì˜ classë§Œì„ ì˜ˆì¸¡í•  ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì‘ì€ object ì—¬ëŸ¬ê°œê°€ ìˆëŠ” ê²½ìš°ì—ëŠ” ì œëŒ€ë¡œ ì˜ˆì¸¡í•˜ì§€ ëª»í•œë‹¤. 2. bounding boxì˜ í˜•íƒœê°€ training dataë¥¼ í†µí•´ì„œë§Œ í•™ìŠµë˜ë¯€ë¡œ ìƒˆë¡œìš´ í˜•íƒœì˜ bounding boxì˜ ê²½ìš° ì •í™•íˆ ì˜ˆì¸¡í•˜ì§€ ëª»í•œë‹¤. 3. ëª‡ ë‹¨ê³„ì˜ layerë¥¼ ê±°ì³ì„œ ë‚˜ì˜¨ feature mapì„ ëŒ€ìƒìœ¼ë¡œ bounding boxë¥¼ ì˜ˆì¸¡í•˜ë¯€ë¡œ localizationì´ ë‹¤ì†Œ ë¶€ì •í™•í•´ì§€ëŠ” ê²½ìš°ê°€ ìˆë‹¤.

ë‹¤ë¥¸ real time object detectionì— ë¹„í•´ ë†’ì€ mAPë¥¼ ë³´ì—¬ì£¼ë©° fast YOLOì˜ ê²½ìš° ê°€ì¥ ë¹ ë¥¸ ì†ë„ì´ë‹¤. Fast R-CNNê³¼ ë¹„êµí•˜ë©´ í›¨ì”¬ ì ì€ false positiveì´ë‹¤. (low background error) Fast R-CNNê³¼ ê°™ì´ ë™ì‘í•˜ë©´ ë³´ì™„í•˜ëŠ” ì—­í• ì„ í•  ìˆ˜ ìˆë‹¤.

### *SSD: Single Shot MultiBox Detector* | [arXiv](https://arxiv.org/abs/1512.02325) 
SSDëŠ” 2015ë…„ì— UNCì˜ Wei Liuê°€ ì œì•ˆí•œ object detection methodë¡œ, single deep neural networkë¥¼ ì´ìš©í•œë‹¤. **Multi-scale feature maps for detection**: ëì´ ì˜ë¦° base networkì— convolutional feature layersë¥¼ ì¶”ê°€í–ˆë‹¤. ì´ layersëŠ” í¬ê¸°ë¥¼ ì ì°¨ ì¤„ì—¬ì„œ ë‹¤ì–‘í•œ í¬ê¸°ì—ì„œ predictionì„ í•œë‹¤. Predicting detectionì„ í•˜ëŠ” convolutional modelì€ feature layerë“¤(Overfeat and YOLO)ê³¼ ë‹¤ë¥´ë‹¤. **Convolutional predictors for detection**

### *EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks* | [arXiv](https://arxiv.org/abs/1905.11946) | [GitHub](https://github.com/tensorflow/tpu/tree/master/models/official/efficientnet)

### *TecoGAN: Learning Temporal Coherence via Self-Supervision for GAN-based Video Generation* | [arXiv](https://arxiv.org/abs/1811.09393) | [GitHub](https://github.com/thunil/TecoGAN)

### *SinGAN: Learning a Generative Model from a Single Natural Image* | [arXiv](https://arxiv.org/pdf/1905.01164.pdf) | [GitHub](https://github.com/FriedRonaldo/SinGAN)
SinGANì€ InGanê³¼ ë§ˆì°¬ê°€ì§€ë¡œ a single natural imageë¡œ ë¶€í„° ì—¬ëŸ¬ imageë¥¼ ìƒì„±í•˜ëŠ” ì—°êµ¬ì´ì§€ë§Œ, ì°¨ì´ì ì€ InGANì€ a single imageì— ëŒ€í•´ì„œ ì—¬ëŸ¬ conditionì„ ì ìš©í–ˆì§€ë§Œ, SinGANì€ unconditionalí•œ ë°©ì‹ì´ë‹¤.

### *InGAN: Capturing and Retargeting the "DNA" of a Natural Image* | [ICCV](https://openaccess.thecvf.com/content_ICCV_2019/papers/Shocher_InGAN_Capturing_and_Retargeting_the_DNA_of_a_Natural_Image_ICCV_2019_paper.pdf) | [Paper (arXiv)](https://arxiv.org/abs/1812.00231)

## ğŸŒ³ AutoML ğŸŒ³

### *NASNet, Learning Transferable Architectures for Scalable Image Recognition* | [arXiv](https://arxiv.org/abs/1707.07012)
2017ë…„ì— Google Brainì˜ Barret Zophì´ ë°œí‘œí•œ í•™ìŠµì„ í†µí•´ modeal architectureë¥¼ ì°¾ëŠ” network modelì´ë‹¤. Reinforcement Learning(RL) searchë¥¼ ì‚¬ìš©í•´ì„œ architectureë¥¼ ìµœì í™”í•˜ëŠ” frameworkë¡œ Neural Architecture Search(NAS) Netì´ë‹¤.

### *PNASNet, Progressive Neural Architecture Search* | [arXiv](https://arxiv.org/abs/1712.00559) | [PyTorch](https://github.com/chenxi116/PNASNet.pytorch)
2017ë…„ì— Google, JHU, Stanfordì—ì„œ ì—°êµ¬í•œ modelë¡œ, model architectureë¥¼ í•™ìŠµí•´ì„œ ìµœì ì˜ model architectureë¥¼ ì°¾ëŠ” modelì´ë‹¤.

### *ENASNet, Efficient Neural Architecture Search via Parameter Sharing* | [arXiv](https://arxiv.org/abs/1802.03268)
2018ë…„ì— Google, CMU, Stanfordì—ì„œ ì—°êµ¬í•œ modelë¡œ, model architectureë¥¼ í•™ìŠµí•´ì„œ ìµœì ì˜ model architectureë¥¼ ì°¾ëŠ” modelì´ë‹¤.

### *MnasNet, Platform-Aware Neural Architecture Search for Mobile* | [arXiv](https://arxiv.org/abs/1807.11626)
2018ë…„ì— Googleì—ì„œ proposed modelë¡œ, mobile environmentì—ì„œ ìµœì ì˜ model architectureë¥¼ ì°¾ëŠ” modelì´ë‹¤. RL searchë¥¼ ì´ìš©í•œë‹¤.

### *YOSO, You Only Search Once: A Fast Automation Framework for Single-Stage DNN/Accelerator Co-design* | [arXiv](https://arxiv.org/abs/2005.07075)

- Precision
Precision measures how accurate is your predictions. The percentage of your predictions are correct
*Precision = TP / (TP + FP)*

- Recall
Recall measures how good you find all the positives. 
*Recall = TP / (TP + FN)*

- IoU(Intersection over Union)
IoU measures the overlap between 2 boundaries.
*IoU = area of overlap / area of union*

- AP(Area under curve AUC)


#### Reference
- Blog KR: [Laon People Machine Learning Academy](https://blog.naver.com/laonple/220463627091)
- Book KR: [ì»´í“¨í„° ë¹„ì „ê³¼ ë”¥ëŸ¬ë‹ (Deep Learning for Computer Vision](http://www.yes24.com/Product/Goods/63830791)
- Book KR: [ì‹¤ì „! í…ì„œí”Œë¡œ 2ë¥¼ í™œìš©í•œ ë”¥ëŸ¬ë‹ ì»´í“¨í„° ë¹„ì „](http://www.yes24.com/Product/Goods/90365150)
- LeNet Blog KR, https://my-coding-footprints.tistory.com/97, 2021-03-10-Wed.
- R-CNN, Fast R-CNN, Faster R-CNN, Mask R-CNN Blog KR, https://tensorflow.blog/2017/06/05/from-r-cnn-to-mask-r-cnn/, 2021-03-05-Fri.
- R-CNN, Fast R-CNN, Faster R-CNN, Mask R-CNN Youtube, https://youtu.be/kcPAGIgBGRs, 2021-03-09-Tue.
- YOLO OpenCV, https://docs.opencv.org/master/da/d9d/tutorial_dnn_yolo.html, 2021-03-05-Fri.
- YOLO Blog KR, https://curt-park.github.io/2017-03-26/yolo/, 2021-03-05-Fri.
- YOLO Blog KR, https://medium.com/curg/you-only-look-once-%EB%8B%A4-%EB%8B%A8%EC%A7%80-%ED%95%9C-%EB%B2%88%EB%A7%8C-%EB%B3%B4%EC%95%98%EC%9D%84-%EB%BF%90%EC%9D%B4%EB%9D%BC%EA%B5%AC-bddc8e6238e2, 2021-02-25-Thu.
- MobileNet Version 1 Blog KR, http://melonicedlatte.com/machinelearning/2019/11/01/212800.html, 2021-03-08-Mon.
- MobileNet Version 2 Blog KR, https://blog.naver.com/PostView.nhn?blogId=chacagea&logNo=221692490366&categoryNo=0&parentCategoryNo=0&viewDate=&currentPage=1&postListTopCurrentPage=1&from=postView, 2021-03-09-Tue.
- MobileNet Version 1 Blog KR, https://blog.naver.com/chacagea/221582912200, 2021-03-09-Tue.
- Selective Search for Object Recognition Slide, http://www.cs.cornell.edu/courses/cs7670/2014sp/slides/VisionSeminar14.pdf, 2021-03-09-Tue.
- Facebook Research Detectron GitHub, https://github.com/facebookresearch/Detectron, 2021-03-09-Tue.
- Facebook Research Detectron2 GitHub, https://github.com/facebookresearch/detectron2, 2021-03-09-Tue.
- OpenMMLab Detection Toolbox and Benchmark GitHub, https://github.com/open-mmlab/mmdetection, 2021-03-09-Tue.
- ResNeXt Blog KR, https://blog.airlab.re.kr/2019/08/resnext, 2021-03-10-Wed.
- ResNeXt Blog KR, https://everyday-deeplearning.tistory.com/entry/%EC%B4%88-%EA%B0%84%EB%8B%A8-%EB%85%BC%EB%AC%B8%EB%A6%AC%EB%B7%B0ResNext, 2021-03-10-Wed.
- SSD Blog KR, https://taeu.github.io/paper/deeplearning-paper-ssd/, 2021-03-15-Mon.
- SSD Blog KR, https://cocopambag.tistory.com/15, 2021-03-15-Mon.
- NASNet Blog KR, https://m.blog.naver.com/PostView.nhn?blogId=za_bc&logNo=221559482028&proxyReferer=https%3A%2F%2Fwww.google.com%2F, 2021-03-15-Mon.
- PNASNet Blog KR, https://m.blog.naver.com/PostView.nhn?blogId=za_bc&logNo=221576139392&proxyReferer=https:%2F%2Fwww.google.com%2F, 2021-03-16-Tue.
- PNASNet GitHub PyTorch, https://github.com/chenxi116/PNASNet.pytorch, 2021-03-16-Tue.
- ENASNet Blog KR, https://m.blog.naver.com/za_bc/221569478807, 2021-03-16-Tue.
- MnasNet Blog KR, https://developers-kr.googleblog.com/2018/09/mnasnet-towards-automating-design-of.html, 2021-03-15-Mon.
- MNasNet Blog KR, https://kmbin93.github.io/deeplearning/2020/07/21/MnasNet/, 2021-03-15-Mon.
- MNasNet Blog KR, https://m.blog.naver.com/PostView.nhn?blogId=za_bc&logNo=221570652712&proxyReferer=https:%2F%2Fwww.google.com%2F, 2021-03-15-Mon.
- mAP(mean Average Precision) for Object Detection Blog US, https://jonathan-hui.medium.com/map-mean-average-precision-for-object-detection-45c121a31173, 2021-03-23-Tue.
